# webrtc-android所有的接口 @see `cc.dot.engine.DotEngine`###实现流程```java        DotEngine.getInstance().setCropVideoView(true/false);//是否每个用户有单独的一个VideoView        //初始化参数相关        DotEngine.getInstance().sharedInstanceWithListener(this,                new DotEngineListener() {                    @Override                    public void onJoined(final String user){                   //用户加入房间                    }                    @Override                    public void onLeave(String user) {                    //用户离开房间                    }                    @Override                    public void onOccurError(DotEngineErrorType errorType) {                    //发生错误                    }                    @Override                    public void onCreateVideoView(GLSurfaceView videoView) {//创建一个共享videoView,这里只有DotEngine.getInstance().setCropVideoView(false)设置为false 的情况才回回调                    }                    @Override                    public void onInitPrepared() {                    //表示webRTC初始化完成了了。只有在该函数回调过了的情况下，才能开始预览和加入房间                        DotEngine.getInstance().startPreview();//开始预览                        DotEngine.getInstance().joinRoom(token);//加入房间                    }                    @Override                    public void onEnableAudio(boolean enable, String user) {                        showToast(user + "音频 " + (enable ? "打开:" : "关闭"));                    }                    @Override                    public void onEnableVideo(boolean enable, String user) {                        showToast(user + "视频 " + (enable ? "打开:" : "关闭"));                    }                    @Override                    public void onAddVideoView(String user, SurfaceView view) {// 创建了某一个子videoView 这里只有DotEngine.getInstance().setCropVideoView(true)设置为true 的情况才回回调                    }                    @Override                    public void onRemoveVideoView(String user, SurfaceView view) {                    //某一个子videoView需要销毁了  这里只有DotEngine.getInstance().setCropVideoView(true)设置为true 的情况才回回调                    }                });        DotEngine.getInstance().setupVideoProfile(DotRTCEngineVideoProfileType.RTCEngine_VideoProfile_360P);//设置自己视频相关参数 包含比特率 分辨率 等        DotEngine.getInstance().startInit();//开始初始化 要跑起来了```暂停预览```DotEngine.getInstance().stopPreview();```离开房间,回收资源```DotEngine.getInstance().leaveRoom();```###Demo说明#####1，Activity简介|activity|说明|优缺点||:------------|:------------|:------------||ShareVideoViewActivity| 只用一个 GLSurfaceView 把所有的的用户视频数据放在这唯一一个控件里面|一个view便于控制，利用率高，但是不灵活||CropVideoActivity |每个用户有自己的单独的SurfaceView|能灵活的放在任何地方，但耗费资源稍大|可以根据业务自行选择,如果房间内的用户展示不是固定的话,建议使用CropVideoActivity#####2，两种模式下的实现的区别单独占有VideoView需要实现回调函数具体见```javacc.dot.engine.listener.DotEngineListener#onAddVideoViewcc.dot.engine.listener.DotEngineListener#onRemoveVideoView``````java                    @Override                    public void onAddVideoView(String user, SurfaceView view) {// 创建了某一个子videoView 这里只有DotEngine.getInstance().setCropVideoView(true)设置为true 的情况才回回调                    }                    @Override                    public void onRemoveVideoView(String user, SurfaceView view) {                    //某一个子videoView需要销毁了  这里只有DotEngine.getInstance().setCropVideoView(true)设置为true 的情况才回回调                    }```共享模式需要实现坐标回调```javaDotEngine.getInstance().setVideoCoordinateListener(new VideoCoordinateListener() {            @Override            public VideoCoordinate getVideoCoordinate(int index, String user, int videoCount, int videoWidth, int videoHeight) {                VideoCoordinate videoCoordinate = null;                try {                //取值范围0-100,可以看作占据屏幕的百分比                //startX,startY 范围在[0,100]                //startX+width 范围在[0,100]                //startY+height 范围在[0,100]                    return new VideoCoordinate((index % 3) * 33, (index / 3) * 33, 33, 33);                } catch (ParamNotSupportException e) {                    e.printStackTrace();                }                return videoCoordinate;            }        });```###需要的权限列表```xml    <uses-sdk        android:minSdkVersion="15"        android:targetSdkVersion="21"/>    <uses-feature android:name="android.hardware.camera"/>    <uses-feature android:name="android.hardware.camera.autofocus"/>    <uses-feature        android:glEsVersion="0x00020000"        android:required="true"/>    <uses-permission android:name="android.permission.CAMERA"/>    <uses-permission android:name="android.permission.MODIFY_AUDIO_SETTINGS"/>    <uses-permission android:name="android.permission.RECORD_AUDIO"/>    <uses-permission android:name="android.permission.INTERNET"/>    <uses-permission android:name="android.permission.WAKE_LOCK"/>    <uses-permission android:name="android.permission.WRITE_EXTERNAL_STORAGE"/>    <uses-permission android:name="android.permission.ACCESS_NETWORK_STATE"/>```####2016年11月06日使用Audio判断```  _audioManagerAndroid.reset(new AudioManager());  // Select best possible combination of audio layers.  if (audioLayer == kPlatformDefaultAudio) {    if (_audioManagerAndroid->IsLowLatencyPlayoutSupported() &&        _audioManagerAndroid->IsLowLatencyRecordSupported()) {      // Use OpenSL ES for both playout and recording.      audioLayer = kAndroidOpenSLESAudio;    } else if (_audioManagerAndroid->IsLowLatencyPlayoutSupported() &&               !_audioManagerAndroid->IsLowLatencyRecordSupported()) {      // Use OpenSL ES for output on devices that only supports the      // low-latency output audio path.      audioLayer = kAndroidJavaInputAndOpenSLESOutputAudio;    } else {      // Use Java-based audio in both directions when low-latency output is      // not supported.      audioLayer = kAndroidJavaAudio;    }  }  AudioManager* audio_manager = _audioManagerAndroid.get();  if (audioLayer == kAndroidJavaAudio) {    // Java audio for both input and output audio.    ptrAudioDevice = new AudioDeviceTemplate<AudioRecordJni, AudioTrackJni>(        audioLayer, audio_manager);  } else if (audioLayer == kAndroidOpenSLESAudio) {    // OpenSL ES based audio for both input and output audio.    ptrAudioDevice = new AudioDeviceTemplate<OpenSLESRecorder, OpenSLESPlayer>(        audioLayer, audio_manager);  } else if (audioLayer == kAndroidJavaInputAndOpenSLESOutputAudio) {    // Java audio for input and OpenSL ES for output audio (i.e. mixed APIs).    // This combination provides low-latency output audio and at the same    // time support for HW AEC using the AudioRecord Java API.    ptrAudioDevice = new AudioDeviceTemplate<AudioRecordJni, OpenSLESPlayer>(        audioLayer, audio_manager);  }```